{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20771fbf",
   "metadata": {},
   "source": [
    "# 10. 앙상블 부스팅(Boosting)\n",
    "\n",
    "## 10.1 핵심 개념\n",
    "\n",
    "**부스팅은 여러개의 약 학습기(weak leaner)를 순차적으로 학습시켜 잘못 예측한 데이터에 가중치를 증가시켜 점진적으로 개선해 나가며 학습하는 앙상블 기법**입니다. 앞서 살펴본 <u>배깅의 경우 여러 데이터셋에서 학습한 결과를 종합하는 병렬식이였다면, 부스팅은 아서 학습한 결과를 두번째 학습에서 수정하고, 세번째 모델에서 또 수정하는등은 순차적 직렬방식의 앙상블</u>이라고 할 수 있습니다.\n",
    "\n",
    "![배깅과부스팅](./extrafiles/boosting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fd6f8",
   "metadata": {},
   "source": [
    "## 10.2 scikit-learn\n",
    "\n",
    "부스팅은 sklearn.ensemble 패키지에 속해 있습니다. 부스팅 메소드로는 AdaBoosting, GradientBoosting 을 지원하고 있습니다.\n",
    "\n",
    "\n",
    "|sklearn.ensemble|Ensemble Methods|\n",
    "|:--|:--|\n",
    "|**skleanr.ensemble.AdaBoostClassifier()** |An AdaBoost classifier. |\n",
    "|**skleanr.ensemble.AdaBoostRegressor()** |An AdaBoost regressor. |\n",
    "|skleanr.ensemble.BaggingClassifier() |A Bagging classifier. |\n",
    "|skleanr.ensemble.BaggingRegressor() |A Bagging regressor. |\n",
    "|skleanr.ensemble.ExtraTreeClassifier() |An extra-tree classifier. |\n",
    "|skleanr.ensemble.ExtraTreeRegressor() |An extra-tree regressor. |\n",
    "|**skleanr.ensemble.GradientBoostingClassifier()** |Gradient Boosting for classification. |\n",
    "|**skleanr.ensemble.GradientBoostingRegressor()** |Gradient Boosting for regressor.|\n",
    "|skleanr.ensemble.IsolationForest() |Isolation Forest Algorithm. |\n",
    "\n",
    "### 가. AdaBoosting\n",
    "\n",
    "AdaBoosting 에서 **분류 알고리즘으로는 AdaBoostingClassifier 와 회귀분석 알고리즘으로 AdaBoostingRegressor** 가 있습니다. 두 방식의 기본 옵션은 동일하여 **base_estimator 와 n_estiator 가 하이퍼파라미터** 입니다.\n",
    "\n",
    "\n",
    "### 나. GradientBoosting\n",
    "\n",
    "GradientBoosting 에서 분류 알고리즘으로는 GradientBoostingClassifier 와 회귀분석 알고리즘으로 GradientBoostingRegressor 가 있습니다. 두 방식의 기본 옵션은 동일하며, Gradient 알고리즘 방식은 학습율(learning_rate)를 설정하여 오차(loss)를 줄이는 방향으로 알고리즘이 진행되기에 옵션 설정이 다고 복잡합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b0d341",
   "metadata": {},
   "source": [
    "## 10.3 분석 코드\n",
    "\n",
    "### Part1. 분류(Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "275c364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['code', 'Clump_Thickness', 'Cell_Size', 'Cell_Shape',\n",
      "       'Marginal_Adhesion', 'Single_Epithelial_Cell_Size', 'Bare_Nuclei',\n",
      "       'Bland_Chromatin', 'Normal_Nucleoli', 'Mitoses', 'Class'],\n",
      "      dtype='object')\n",
      "Class    0.349609\n",
      "dtype: float64\n",
      "Class    0.350877\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 경고레벨조정\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 데이터 로드\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"./extrafiles/breast-cancer-wisconsin.csv\", encoding='utf-8')\n",
    "\n",
    "# 컬럼정보 확인\n",
    "print(data.columns)\n",
    "\n",
    "# 독립변수/ 종속변수 분리\n",
    "X = data[['Clump_Thickness', 'Cell_Size', 'Cell_Shape',\n",
    "       'Marginal_Adhesion', 'Single_Epithelial_Cell_Size', 'Bare_Nuclei',\n",
    "       'Bland_Chromatin', 'Normal_Nucleoli', 'Mitoses']]\n",
    "y = data[['Class']]\n",
    "\n",
    "# train-test data 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "# stratify 효과 - 범주형 변수를 유사한 비율로 train / test 데이터로 분리시켜 준다.\n",
    "print(y_train.mean())\n",
    "print(y_test.mean())\n",
    "\n",
    "# 표준화 작업 - MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_scaled_train = scaler.transform(X_train)\n",
    "X_scaled_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8566ff",
   "metadata": {},
   "source": [
    "**[AdaBoosting 앙상블 모델 적용]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c04617e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AdaBoosting 앙상블 모델 적용\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = AdaBoostClassifier(n_estimators = 100, random_state = 0)\n",
    "model.fit(X_scaled_train, y_train)\n",
    "pred_train = model.predict(X_scaled_train)\n",
    "model.score(X_scaled_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acba785d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련데이터 오차행렬 : \n",
      " [[333   0]\n",
      " [  0 179]]\n"
     ]
    }
   ],
   "source": [
    "# 훈련데이터의 혼동행렬 작성\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_train = confusion_matrix(y_train, pred_train)\n",
    "print(\"훈련데이터 오차행렬 : \\n\", confusion_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "407f8788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류예측 레포트 : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       333\n",
      "           1       1.00      1.00      1.00       179\n",
      "\n",
      "    accuracy                           1.00       512\n",
      "   macro avg       1.00      1.00      1.00       512\n",
      "weighted avg       1.00      1.00      1.00       512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 훈련데이터의 분류 레포트 작성\n",
    "from sklearn.metrics import classification_report\n",
    "cfreport_train = classification_report(y_train, pred_train)\n",
    "print(\"분류예측 레포트 : \\n\", cfreport_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae26deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9532163742690059"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트데이터 예측결과 생성\n",
    "pred_test = model.predict(X_scaled_test)\n",
    "model.score(X_scaled_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84374271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트데이터 오차행렬 : \n",
      " [[106   5]\n",
      " [  3  57]]\n"
     ]
    }
   ],
   "source": [
    "# 테스트데이터의 혼동행렬 작성\n",
    "confusion_test = confusion_matrix(y_test, pred_test)\n",
    "print(\"테스트데이터 오차행렬 : \\n\", confusion_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6539ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류예측 레포트 : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       111\n",
      "           1       0.92      0.95      0.93        60\n",
      "\n",
      "    accuracy                           0.95       171\n",
      "   macro avg       0.95      0.95      0.95       171\n",
      "weighted avg       0.95      0.95      0.95       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트데이터의 분류 레포트 작성\n",
    "from sklearn.metrics import classification_report\n",
    "cfreport_test = classification_report(y_test, pred_test)\n",
    "print(\"분류예측 레포트 : \\n\", cfreport_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ef84f",
   "metadata": {},
   "source": [
    "**[GradientBoosting 앙상블 모델 적용]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c42cb388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GradientBoosting 앙상블 모델 적용\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate = 1.0, max_depth=1, random_state=0)\n",
    "model.fit(X_scaled_train, y_train)\n",
    "pred_train = model.predict(X_scaled_train)\n",
    "model.score(X_scaled_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4f21e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련데이터 오차행렬 : \n",
      " [[333   0]\n",
      " [  0 179]]\n"
     ]
    }
   ],
   "source": [
    "# 훈련데이터의 혼동행렬 작성\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_train = confusion_matrix(y_train, pred_train)\n",
    "print(\"훈련데이터 오차행렬 : \\n\", confusion_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fce74675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9649122807017544"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트데이터 예측결과 생성\n",
    "pred_test = model.predict(X_scaled_test)\n",
    "model.score(X_scaled_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9ee42a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트데이터 오차행렬 : \n",
      " [[106   5]\n",
      " [  1  59]]\n"
     ]
    }
   ],
   "source": [
    "# 테스트데이터의 혼동행렬 작성\n",
    "confusion_test = confusion_matrix(y_test, pred_test)\n",
    "print(\"테스트데이터 오차행렬 : \\n\", confusion_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3a9cc5",
   "metadata": {},
   "source": [
    "**[종합정리]**\n",
    "\n",
    "부스팅 방법은 오류를 찾아 해결하는 방식으로 해를 찾으르모 훈련데이터에 과적합되는 경향을 보입니다. AdaBoosting, GradientBoosting 모두 과적합 문제가 나타나지만 테스트 데이터에서는 일반적인 수준의 정확도를 보이고 있습니다. SCV 알고리즘외 다른 알고리즘을 사용한다면 보다 좋은 결과도 기대할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecb5710",
   "metadata": {},
   "source": [
    "### Part2. 회귀(Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7131ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['housing_age', 'income', 'bedrooms', 'households', 'rooms',\n",
      "       'house_value'],\n",
      "      dtype='object')\n",
      "Index(['income', 'bedrooms', 'households', 'rooms'], dtype='object')\n",
      "house_value    189260.967812\n",
      "dtype: float64\n",
      "house_value    188391.001357\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "data2 = pd.read_csv('./extrafiles/house_price.csv', encoding='utf-8')\n",
    "\n",
    "print(data2.columns)\n",
    "\n",
    "X = data2[data2.columns[1:5]]\n",
    "y = data2[['house_value']]\n",
    "\n",
    "print(X.columns)\n",
    "\n",
    "# train-test data 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# stratify 효과 - 범주형 변수를 유사한 비율로 train / test 데이터로 분리시켜 준다.\n",
    "print(y_train.mean())\n",
    "print(y_test.mean())\n",
    "\n",
    "# 표준화 작업 - MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_scaled_train = scaler.transform(X_train)\n",
    "X_scaled_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218260b5",
   "metadata": {},
   "source": [
    "**[AdaBoosting 앙상블 모델 적용]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29c4a47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4353130085971758"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AdaBoosting 앙상블 모델 적용\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "model = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "model.fit(X_scaled_train, y_train)\n",
    "pred_train = model.predict(X_scaled_train)\n",
    "model.score(X_scaled_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "991775ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43568387094087124"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터 모델 적용\n",
    "pred_test = model.predict(X_scaled_test)\n",
    "model.score(X_scaled_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6394be89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈  련데이터 RMSE: 71722.42012035428\n",
      "테스트데이터 RMSE: 71816.41231019037\n"
     ]
    }
   ],
   "source": [
    "# 회귀분석의 지표 R Square n RMSE\n",
    "# RMSE (Root Mean Squared Error)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "MSE_train = mean_squared_error(y_train, pred_train)\n",
    "MSE_test = mean_squared_error(y_test, pred_test)\n",
    "print(\"훈  련데이터 RMSE:\", np.sqrt(MSE_train))\n",
    "print(\"테스트데이터 RMSE:\", np.sqrt(MSE_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca997b",
   "metadata": {},
   "source": [
    "**[GradientBoosting 앙상블 모델 적용]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71440c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6178724780500952"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor(random_state=0)\n",
    "model.fit(X_scaled_train, y_train)\n",
    "pred_train = model.predict(X_scaled_train)\n",
    "model.score(X_scaled_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be17a070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5974112241813845"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터 모델 적용\n",
    "pred_test = model.predict(X_scaled_test)\n",
    "model.score(X_scaled_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fcd84e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈  련데이터 RMSE: 59000.433545962376\n",
      "테스트데이터 RMSE: 60658.72886338227\n"
     ]
    }
   ],
   "source": [
    "# 회귀분석의 지표 R Square n RMSE\n",
    "# RMSE (Root Mean Squared Error)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "MSE_train = mean_squared_error(y_train, pred_train)\n",
    "MSE_test = mean_squared_error(y_test, pred_test)\n",
    "print(\"훈  련데이터 RMSE:\", np.sqrt(MSE_train))\n",
    "print(\"테스트데이터 RMSE:\", np.sqrt(MSE_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb105ff",
   "metadata": {},
   "source": [
    "**[종합정리]**\n",
    "\n",
    "회귀문제에서는 기본 모델을 설정한 경우 GradientBoosting 이 AdaBoosting 보다 더 좋은 성능을 보여주었다. 하지만 일반적인 경우 하이퍼파라미터를 조정 하는 경우 AdaBoosting 이 보다 더 좋은 결과를 낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8683ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
